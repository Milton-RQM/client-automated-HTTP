# Pipeline de Ingesti√≥n HTTP, Procesamiento de KPIs y ETL

## üìã Tabla de contenidos

1. [Descripci√≥n general](#descripci√≥n-general)
2. [Requisitos del proyecto](#requisitos-del-proyecto)
3. [Instalaci√≥n y configuraci√≥n](#instalaci√≥n-y-configuraci√≥n)
4. [Estructura del proyecto](#estructura-del-proyecto)
5. [C√≥mo ejecutar el pipeline](#c√≥mo-ejecutar-el-pipeline)
6. [M√≥dulos principales](#m√≥dulos-principales)
7. [Flujo de datos completo](#flujo-de-datos-completo)
8. [Troubleshooting](#troubleshooting)
9. [Tecnolog√≠as utilizadas](#tecnolog√≠as-utilizadas)
10. [FAQ](#faq)
11. [Licencia](#-licencia)
12. [Contacto](#Ô∏è-contacto)
13. [Referencias](#-referencias)
14. [Aprendizajes clave](#-aprendizajes-clave)
15. [Mejoras futuras](#-mejoras-futuras)

---

## üìñ Descripci√≥n general

Este proyecto implementa un **pipeline de datos end-to-end** que:

1. **Simula ingesti√≥n HTTP** desde APIs (httpbin.org)
2. **Genera logs sint√©ticos** con patrones realistas de tr√°fico
3. **Procesa datos** y calcula KPIs diarios por endpoint
4. **Carga datos** en base de datos usando ETL (Pentaho)
5. **Genera reportes** HTML con visualizaciones

### üéØ Objetivos

- Consumir endpoints HTTP con autenticaci√≥n, cookies, redirecciones, etc.
---

## üìã Requisitos del proyecto

### Requisitos de negocio

**Tareas de Ingesti√≥n HTTP:**
- ‚úÖ Autenticaci√≥n b√°sica (usuario_test / clave123)
- ‚úÖ Manejo de cookies y sesiones
- ‚úÖ Simulaci√≥n de restricciones (403)
- ‚úÖ Extracci√≥n de datos en JSON, XML, HTML
- ‚úÖ Simulaci√≥n de env√≠o de formularios
- ‚úÖ Manejo de redirecciones

**Procesamiento de datos:**
- ‚úÖ Generaci√≥n de 500+ logs sint√©ticos
- ‚úÖ C√°lculo de KPIs diarios (requests, √©xitos, errores, latencia, percentiles)
- ‚úÖ Normalizaci√≥n de endpoints

**Reportes:**
- ‚úÖ HTML con tablas y gr√°ficos
- ‚úÖ M√©tricas globales por endpoint
- ‚úÖ Alertas de rendimiento

### Requisitos t√©cnicos

| Requerimiento | Versi√≥n |
|--------------|---------|
| Python | 3.8+ |
| requests | 2.31.0+ |
| faker | 20.0.0+ |
| beautifulsoup4 | 4.12.0+ |
| lxml | 4.9.0+ |
| pandas | 2.0.0+ |
| numpy | 1.24.0+ |
| matplotlib | 3.7.0+ |

**Opcional (para ETL):**
- Pentaho Data Integration 9.0+
- SQLite 3.0+

---

## ‚öôÔ∏è Instalaci√≥n y configuraci√≥n

### 1. Clonar o descargar el proyecto

```bash
# Si usas git
git clone <URL_DEL_REPO>
cd client-automated-HTTP

# Si descargaste un ZIP, descomprimelo y abre la carpeta
cd client-automated-HTTP
```

### 2. Crear un ambiente virtual (recomendado)

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux / macOS
python3 -m venv venv
source venv/bin/activate
```

### 3. Instalar dependencias

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 4. Validar instalaci√≥n

```bash
python setup_and_validate.py
```

**Salida esperada:**
```
‚úì Python version >= 3.8
‚úì All required packages installed
‚úì Directory structure OK
‚úì Main files present
‚úì requirements.txt found

‚úì El proyecto est√° listo para ejecutar.
```

### 5. (Opcional) Configurar Pentaho

Si deseas usar el m√≥dulo de ETL con Pentaho:

1. Descarga Pentaho Data Integration (PDI) desde [pentaho.com](https://www.pentaho.com)
2. Extrae en `04_etl_pentaho/`
3. Los archivos `.ktr` y `.kjb` ya est√°n en el proyecto

---

## üìÅ Estructura del proyecto

```
client-automated-HTTP/
‚îÇ
‚îú‚îÄ‚îÄ 01_ingestion_http/              # M√≥dulo 1: Ingesti√≥n HTTP
‚îÇ   ‚îú‚îÄ‚îÄ run_all.py                  # Script principal
‚îÇ   ‚îú‚îÄ‚îÄ auth/                       # Autenticaci√≥n b√°sica
‚îÇ   ‚îú‚îÄ‚îÄ cookies/                    # Cookies y sesiones
‚îÇ   ‚îú‚îÄ‚îÄ extraction/                 # JSON, XML, HTML
‚îÇ   ‚îú‚îÄ‚îÄ forms/                      # Formularios POST
‚îÇ   ‚îú‚îÄ‚îÄ errors/                     # Manejo de 403
‚îÇ   ‚îú‚îÄ‚îÄ redirects/                  # Redirecciones
‚îÇ   ‚îî‚îÄ‚îÄ out/                        # Salidas
‚îÇ
‚îú‚îÄ‚îÄ 02_simulation_logs/             # M√≥dulo 2: Logs sint√©ticos
‚îÇ   ‚îú‚îÄ‚îÄ generar_datos.py            # Generador JSONL
‚îÇ   ‚îî‚îÄ‚îÄ out/                        # Logs generados
‚îÇ
‚îú‚îÄ‚îÄ 03_kpi_processing/              # M√≥dulo 3: KPIs
‚îÇ   ‚îú‚îÄ‚îÄ calcular_kpis.py            # Calculador de KPIs
‚îÇ   ‚îú‚îÄ‚îÄ README.md                   # Documentaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ out/                        # KPIs (CSV)
‚îÇ
‚îú‚îÄ‚îÄ 04_etl_pentaho/                 # M√≥dulo 4: ETL
‚îÇ   ‚îú‚îÄ‚îÄ t_load_kpi.ktr              # Transformaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ j_daily_kpi.kjb             # Job
‚îÇ   ‚îú‚îÄ‚îÄ db/                         # Base de datos
‚îÇ   ‚îî‚îÄ‚îÄ logs/                       # Logs
‚îÇ
‚îú‚îÄ‚îÄ 05_reporting/                   # M√≥dulo 5: Reportes
‚îÇ   ‚îú‚îÄ‚îÄ generar_reporte.py          # Generador HTML
‚îÇ   ‚îî‚îÄ‚îÄ out/                        # Reportes
‚îÇ
‚îú‚îÄ‚îÄ setup_and_validate.py           # Validaci√≥n del ambiente
‚îú‚îÄ‚îÄ requirements.txt                # Dependencias
‚îî‚îÄ‚îÄ README.md                       # Este archivo
```

---

## üöÄ C√≥mo ejecutar el pipeline

### Ejecuci√≥n completa (paso a paso)

```bash
# 1. Validar que todo est√° instalado
python setup_and_validate.py

# 2. Generar datos simulados
python 02_simulation_logs/generar_datos.py \
  --n_registros 500 \
  --seed 42

# 3. Calcular KPIs
python 03_kpi_processing/calcular_kpis.py \
  --input 02_simulation_logs/out/http_logs.jsonl \
  --output 03_kpi_processing/out/kpi_por_endpoint_dia.csv

# 4. Generar reporte HTML
python 05_reporting/generar_reporte.py \
  --input 03_kpi_processing/out/kpi_por_endpoint_dia.csv \
  --output 05_reporting/out/report/kpi_diario.html
```

### Ejecuci√≥n con valores por defecto

```bash
# Generar datos (500 registros, seed 42)
python 02_simulation_logs/generar_datos.py

# Procesar KPIs
python 03_kpi_processing/calcular_kpis.py

# Generar reporte
python 05_reporting/generar_reporte.py
```

---

## üìä M√≥dulos principales

### M√≥dulo 02: Generaci√≥n de logs

Genera archivo JSONL con registros sint√©ticos de tr√°fico HTTP.

```bash
python 02_simulation_logs/generar_datos.py \
  --n_registros 1000 \
  --seed 42
```

**Par√°metros:**
- `--n_registros`: N√∫mero de registros (default: 500)
- `--seed`: Semilla para reproducibilidad (default: 42)
- `--salida`: Ruta de salida (default: out/http_logs.jsonl)

---

### M√≥dulo 03: Procesamiento de KPIs

Calcula indicadores de rendimiento diarios por endpoint.

```bash
python 03_kpi_processing/calcular_kpis.py \
  --input 02_simulation_logs/out/http_logs.jsonl \
  --output 03_kpi_processing/out/kpi_por_endpoint_dia.csv
```

**M√©tricas:**
- `requests_total`: Total de solicitudes
- `success_2xx`: Solicitudes exitosas (200-299)
- `client_4xx`: Errores del cliente (400-499)
- `server_5xx`: Errores del servidor (500-599)
- `parse_errors`: Registros con parse_result != "ok"
- `avg_elapsed_ms`: Tiempo promedio de respuesta
- `p90_elapsed_ms`: Percentil 90 de tiempo de respuesta

---

### M√≥dulo 05: Reportes

Genera reporte HTML interactivo con tablas y gr√°ficos.

```bash
python 05_reporting/generar_reporte.py \
  --input 03_kpi_processing/out/kpi_por_endpoint_dia.csv \
  --output 05_reporting/out/report/kpi_diario.html \
  --umbral_p90 300
```

**Ver el reporte:**
```bash
# Windows
start 05_reporting/out/report/kpi_diario.html

# Linux
xdg-open 05_reporting/out/report/kpi_diario.html

# macOS
open 05_reporting/out/report/kpi_diario.html
```

---

## üîÑ Flujo de datos completo

**Diagrama del Pipeline:**

```
M√ìDULO 01          M√ìDULO 02           M√ìDULO 03          M√ìDULO 04/05
Ingesti√≥n HTTP     Generaci√≥n Logs     KPI Processing     ETL + Reporting

httpbin.org
    |
    v
generar_datos.py
    |
    v
http_logs.jsonl
    |
    +-----------> calcular_kpis.py
                      |
                      v
                  kpi_por_endpoint_dia.csv
                      |
                      +-----------> Pentaho ETL
                      |                 |
                      |                 v
                      |           SQLite Database
                      |           (stg + fct tables)
                      |
                      +-----------> generar_reporte.py
                                        |
                                        v
                                    kpi_diario.html
```

---

### üì∏ Diagramas de ETL (Pentaho)

**Nota:** Agrega aqu√≠ tus capturas de pantalla de Pentaho Spoon:

**1. Transformaci√≥n (t_load_kpi.ktr):**

```
![Transformaci√≥n t_load_kpi](04_etl_pentaho/diagramas/img_1.png)

Instrucciones para agregar la imagen:
1. Abre la transformaci√≥n en Pentaho Spoon
2. Presiona Print Screen
3. Guarda la imagen como: 04_etl_pentaho/diagramas/img_1.png
4. Para mostrar otra imagen reemplaza el path en la l√≠nea anterior
```

**2. Job (j_daily_kpi.kjb):**

```
[Espacio para agregar captura de Spoon]

Instrucciones para agregar la imagen:
1. Abre el job en Pentaho Spoon
2. Presiona Print Screen
3. Guarda la imagen como: 04_etl_pentaho/diagramas/j_daily_kpi.png
4. Descomenta la l√≠nea siguiente:
   ![Job j_daily_kpi](04_etl_pentaho/diagramas/j_daily_kpi.png)
```

---

### Detalle: ETL con Pentaho (M√≥dulo 04)

**Transformacion t_load_kpi.ktr (Pasos ejecutados):**

1. **CSV Input** ‚Üí Lee archivo `kpi_por_endpoint_dia.csv`
2. **Type Casting** ‚Üí Convierte tipos (fecha, entero, decimal)
3. **Filter Rows** ‚Üí Valida integridad de datos:
   - `requests_total > 0`
   - `p90_elapsed_ms >= avg_elapsed_ms`
4. **Table Output #1** ‚Üí Carga tabla staging (`stg_kpi_endpoint_dia`)
5. **Table Output #2** ‚Üí Carga tabla fact (`fct_kpi_endpoint_dia`)

**Job j_daily_kpi.kjb (Flujo completo):**

| Paso | Accion | Validacion |
|------|--------|------------|
| 1 | Ejecuta transformacion t_load_kpi.ktr | Verifica exito |
| 2 | Valida numero de filas en stg_kpi | Coincide con CSV |
| 3 | Valida numero de filas en fct_kpi | Igual a staging |
| 4 | Registra en log de auditoria | Timestamp + conteo |
| 5 | Envia email si hay errores | Opcional |

---

### SQLite Database (M√≥dulo 04)

**Tabla Staging: stg_kpi_endpoint_dia**

```sql
CREATE TABLE stg_kpi_endpoint_dia (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  date_utc TEXT NOT NULL,
  endpoint_base TEXT NOT NULL,
  requests_total INTEGER NOT NULL,
  success_2xx INTEGER,
  client_4xx INTEGER,
  server_5xx INTEGER,
  parse_errors INTEGER DEFAULT 0,
  avg_elapsed_ms REAL,
  p90_elapsed_ms REAL,
  loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_stg_date ON stg_kpi_endpoint_dia(date_utc);
CREATE INDEX idx_stg_endpoint ON stg_kpi_endpoint_dia(endpoint_base);
```

**Tabla Fact: fct_kpi_endpoint_dia**

```sql
CREATE TABLE fct_kpi_endpoint_dia (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  date_utc TEXT NOT NULL,
  endpoint_base TEXT NOT NULL,
  requests_total INTEGER NOT NULL,
  success_2xx INTEGER,
  client_4xx INTEGER,
  server_5xx INTEGER,
  parse_errors INTEGER DEFAULT 0,
  avg_elapsed_ms REAL,
  p90_elapsed_ms REAL,
  loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_fct_date ON fct_kpi_endpoint_dia(date_utc);
CREATE INDEX idx_fct_endpoint ON fct_kpi_endpoint_dia(endpoint_base);
```

**Tabla de Auditoria: audit_etl_log**

```sql
CREATE TABLE audit_etl_log (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  job_name TEXT,
  execution_date TIMESTAMP,
  status TEXT,
  records_processed INTEGER,
  error_message TEXT,
  duration_seconds REAL
);
```

**Para crear las tablas en tu SQLite:**

```bash
sqlite3 04_etl_pentaho/kpis.db < 04_etl_pentaho/schema.sql
```

-- Auditor√≠a: registro de cada carga
audit_etl_log (
  id, job_name, execution_date, records_loaded,
  records_expected, status, error_message
)
```

**Crear tablas:**
```bash
sqlite3 04_etl_pentaho/db/pipeline.db < 04_etl_pentaho/create_tables.sql
```

---

---

## üö® Troubleshooting

### "ModuleNotFoundError: No module named 'pandas'"

```bash
pip install -r requirements.txt
```

### "FileNotFoundError: No existe el input JSONL"

```bash
cd 02_simulation_logs
python generar_datos.py --salida out/http_logs.jsonl
```

### "JSON mal formado en l√≠nea X"

```bash
# Regenera los datos
python 02_simulation_logs/generar_datos.py --n_registros 100
```

### "No existe el archivo de reporte"

```bash
mkdir -p 05_reporting/out/report
python 05_reporting/generar_reporte.py
```

---

## üìö Tecnolog√≠as utilizadas

| Categor√≠a | Tecnolog√≠a |
|-----------|------------|
| Backend | Python 3.8+ |
| HTTP | requests |
| Parsing | beautifulsoup4, lxml |
| Datos sint√©ticos | Faker |
| An√°lisis | pandas, numpy |
| Visualizaci√≥n | matplotlib |
| ETL | Pentaho Data Integration (opcional) |
| BD | SQLite (opcional) |

---

## üìû FAQ

**¬øPuedo usar esto en producci√≥n?**  
S√≠. Los m√≥dulos 02, 03, 05 est√°n listos. El m√≥dulo 04 requiere instalaci√≥n de Pentaho.

**¬øC√≥mo cambio el umbral de p90?**  
```bash
python 05_reporting/generar_reporte.py --umbral_p90 500
```

**¬øQu√© significa p90_elapsed_ms?**  
Es el percentil 90 del tiempo de respuesta. El 90% de solicitudes tard√≥ menos que este valor.

**¬øC√≥mo a√±ado m√°s endpoints?**  
Edita `02_simulation_logs/generar_datos.py` y modifica la lista `ENDPOINTS`.

---

## üìÑ Licencia

Este proyecto fue desarrollado como parte de una **prueba t√©cnica** de Data Engineering.

Derechos de uso:
- ‚úÖ Uso educativo y de demostraci√≥n
- ‚úÖ Modificaci√≥n y distribuci√≥n con atribuci√≥n
- ‚úÖ Uso en entornos de desarrollo y testing
- ‚ö†Ô∏è No incluye garant√≠as de soporte para producci√≥n

---

## ‚úâÔ∏è Contacto

**Desarrollador:** Milton RQM  
**GitHub:** [@Milton-RQM](https://github.com/Milton-RQM)  
**Proyecto:** [client-automated-HTTP](https://github.com/Milton-RQM/client-automated-HTTP)

Para preguntas o sugerencias:
- üìß Email: milton.rdqm@gmail.com
- üí¨ Issues: Abre un issue en el repositorio de GitHub
- üêõ Bugs: Reporta en la secci√≥n de Issues

---

## üìö Referencias

### Documentaci√≥n oficial

- **httpbin.org**: [httpbin.org/docs](https://httpbin.org/docs)
- **Python Requests**: [docs.python-requests.org](https://docs.python-requests.org/)
- **Beautiful Soup**: [bs4.readthedocs.io](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- **Faker (datos sint√©ticos)**: [faker.readthedocs.io](https://faker.readthedocs.io/)
- **Pandas**: [pandas.pydata.org](https://pandas.pydata.org/)
- **NumPy**: [numpy.org](https://numpy.org/)
- **Matplotlib**: [matplotlib.org](https://matplotlib.org/)

### Pentaho Data Integration

- **PDI Documentation**: [help.hitachivantara.com/Pentaho DI](https://help.hitachivantara.com/Documentation/Software/Pentaho/9.0)
- **Spoon User Guide**: [Gu√≠a de usuario de Spoon](https://help.hitachivantara.com/Documentation/Software/Pentaho/9.0/en)
- **Instalaci√≥n**: [pentaho.com/download](https://www.pentaho.com/download)

### SQLite

- **SQLite Docs**: [sqlite.org/docs.html](https://www.sqlite.org/docs.html)
- **Tutorial SQL**: [w3schools.com/sql](https://www.w3schools.com/sql/)
- **Herramientas GUI**: [sqlitebrowser.org](https://sqlitebrowser.org/)

### Conceptos de Data Engineering

- **ETL Concepts**: [en.wikipedia.org/wiki/Extract,_transform,_load](https://en.wikipedia.org/wiki/Extract,_transform,_load)
- **KPIs**: [en.wikipedia.org/wiki/Key_performance_indicator](https://en.wikipedia.org/wiki/Key_performance_indicator)
- **Percentiles**: [en.wikipedia.org/wiki/Percentile](https://en.wikipedia.org/wiki/Percentile)
- **Data Normalization**: [en.wikipedia.org/wiki/Database_normalization](https://en.wikipedia.org/wiki/Database_normalization)

### Herramientas √∫tiles

- **VS Code**: [code.visualstudio.com](https://code.visualstudio.com)
- **SQLite Browser**: [sqlitebrowser.org](https://sqlitebrowser.org/)
- **Postman (testing HTTP)**: [postman.com](https://www.postman.com/)
- **Git**: [git-scm.com](https://git-scm.com/)

---

## üéì Aprendizajes clave

Este proyecto demuestra:

1. **Integraci√≥n de APIs**: Consumo de endpoints HTTP con requests
2. **Calidad de datos**: Validaci√≥n, normalizaci√≥n y limpieza
3. **Ingenier√≠a ETL**: Pipelines automatizados con Pentaho
4. **An√°lisis de datos**: KPIs, percentiles, agregaciones
5. **Visualizaci√≥n**: Gr√°ficos y reportes HTML interactivos
6. **Automatizaci√≥n**: Scripts reproducibles con par√°metros CLI
7. **Buenas pr√°cticas**: Documentaci√≥n, errores, validaci√≥n

---

## ‚ú® Mejoras futuras

- [ ] Agregar autenticaci√≥n a API de reportes
- [ ] Implementar dashboard en tiempo real con Plotly
- [ ] Integraci√≥n con Apache Airflow para orquestaci√≥n
- [ ] Alertas autom√°ticas por email en casos de anomal√≠as
- [ ] Hist√≥rico de KPIs con consultas de tendencias
- [ ] API REST para consultar KPIs
- [ ] Dockerizaci√≥n del pipeline completo
- [ ] Pruebas unitarias y de integraci√≥n

---

**√öltima actualizaci√≥n:** 2026-02-06  
**Versi√≥n:** 1.0.0  
**Estado:** ‚úÖ Listo para ejecuci√≥n en cualquier equipo con Python 3.8+

‚îÇ   ‚îú‚îÄ xml/
‚îÇ   ‚îî‚îÄ html/
‚îÇ
‚îî‚îÄ run_all.py

