# Pipeline de IngestiÃ³n HTTP, Procesamiento de KPIs y ETL

## ğŸ“‹ Tabla de contenidos

1. [DescripciÃ³n general](#-descripciÃ³n-general)
2. [Requisitos del proyecto](#-requisitos-del-proyecto)
3. [InstalaciÃ³n y configuraciÃ³n](#%EF%B8%8F-instalaciÃ³n-y-configuraciÃ³n)
4. [Estructura del proyecto](#-estructura-del-proyecto)
5. [CÃ³mo ejecutar el pipeline](#-cÃ³mo-ejecutar-el-pipeline)
6. [MÃ³dulos principales](#-mÃ³dulos-principaless)
7. [Flujo de datos completo](#-flujo-de-datos-completo)
8. [Troubleshooting](#-troubleshooting)
9. [TecnologÃ­as utilizadas](#-tecnologÃ­as-utilizadas)
10. [FAQ](#-faq)
11. [Licencia](#-licencia)
12. [Contacto](#-contacto)
13. [Referencias](#-referencias)
14. [Aprendizajes clave](#-aprendizajes-clave)
15. [Mejoras futuras](#-mejoras-futuras)

---

## ğŸ“– DescripciÃ³n general

Este proyecto implementa un **pipeline de datos end-to-end** que:

1. **Simula ingestiÃ³n HTTP** desde APIs (httpbin.org)
2. **Genera logs sintÃ©ticos** con patrones realistas de trÃ¡fico
3. **Procesa datos** y calcula KPIs diarios por endpoint
4. **Carga datos** en base de datos usando ETL (Pentaho)
5. **Genera reportes** HTML con visualizaciones

### ğŸ¯ Objetivos

- Consumir endpoints HTTP con autenticaciÃ³n, cookies, redirecciones, etc.
---

## ğŸ“‹ Requisitos del proyecto

### Requisitos de negocio

**Tareas de IngestiÃ³n HTTP:**
- âœ… AutenticaciÃ³n bÃ¡sica (usuario_test / clave123)
- âœ… Manejo de cookies y sesiones
- âœ… SimulaciÃ³n de restricciones (403)
- âœ… ExtracciÃ³n de datos en JSON, XML, HTML
- âœ… SimulaciÃ³n de envÃ­o de formularios
- âœ… Manejo de redirecciones

**Procesamiento de datos:**
- âœ… GeneraciÃ³n de 500+ logs sintÃ©ticos
- âœ… CÃ¡lculo de KPIs diarios (requests, Ã©xitos, errores, latencia, percentiles)
- âœ… NormalizaciÃ³n de endpoints

**Reportes:**
- âœ… HTML con tablas y grÃ¡ficos
- âœ… MÃ©tricas globales por endpoint
- âœ… Alertas de rendimiento

### Requisitos tÃ©cnicos

| Requerimiento | VersiÃ³n |
|--------------|---------|
| Python | 3.8+ |
| requests | 2.31.0+ |
| faker | 20.0.0+ |
| beautifulsoup4 | 4.12.0+ |
| lxml | 4.9.0+ |
| pandas | 2.0.0+ |
| numpy | 1.24.0+ |
| matplotlib | 3.7.0+ |

**Opcional (para ETL):**
- Pentaho Data Integration 9.0+
- SQLite 3.0+

---

## âš™ï¸ InstalaciÃ³n y configuraciÃ³n

### 1. Clonar o descargar el proyecto

```bash
# Si usas git
git clone <URL_DEL_REPO>
cd client-automated-HTTP

# Si descargaste un ZIP, descomprimelo y abre la carpeta
cd client-automated-HTTP
```

### 2. Crear un ambiente virtual (recomendado)

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux / macOS
python3 -m venv venv
source venv/bin/activate
```

### 3. Instalar dependencias

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 3.1 Uso de credenciales (variables de entorno)

Las credenciales sensibles usadas por los scripts de ingestiÃ³n NO deben subirse a GitHub. El proyecto soporta variables de entorno y un archivo local `.env` (no versionado).

- Copia el ejemplo y rellena tus datos:

```bash
copy .env.example .env        # Windows (PowerShell / cmd)
# cp .env.example .env       # Linux / macOS
```

- `README` y los scripts usan estas variables:
  - `INGESTION_BASE_URL` (opcional)
  - `INGESTION_BASIC_USER`
  - `INGESTION_BASIC_PASS`

- AsegÃºrate de que `.env` estÃ© en `.gitignore` (ya estÃ¡ configurado). No aÃ±adas credenciales al repositorio.

- Alternativa sin archivo `.env`: exporta las variables en la sesiÃ³n de terminal:

```bash
# Windows (PowerShell)
$Env:INGESTION_BASIC_USER = "usuario_test"
$Env:INGESTION_BASIC_PASS = "clave123"

# Linux / macOS
export INGESTION_BASIC_USER=usuario_test
export INGESTION_BASIC_PASS=clave123
```

- Prueba local rÃ¡pida del script de autenticaciÃ³n (usa las variables definidas arriba):

```bash
python 01_ingestion_http/auth/basic_auth.py
```

El archivo `01_ingestion_http/auth/basic_auth.py` carga automÃ¡ticamente `.env` si tienes `python-dotenv` instalado (aÃ±adido en `requirements.txt`).

### 4. Validar instalaciÃ³n

```bash
python setup_and_validate.py
```

**Salida esperada:**
```
âœ“ Python version >= 3.8
âœ“ All required packages installed
âœ“ Directory structure OK
âœ“ Main files present
âœ“ requirements.txt found

âœ“ El proyecto estÃ¡ listo para ejecutar.
```

### 5. (Opcional) Configurar Pentaho

Si deseas usar el mÃ³dulo de ETL con Pentaho:

1. Descarga Pentaho Data Integration (PDI) desde [pentaho.com](https://www.pentaho.com)
2. Extrae en `04_etl_pentaho/`
3. Los archivos `.ktr` y `.kjb` ya estÃ¡n en el proyecto

---

## ğŸ“ Estructura del proyecto

```
client-automated-HTTP/
â”‚
â”œâ”€â”€ 01_ingestion_http/              # MÃ³dulo 1: IngestiÃ³n HTTP
â”‚   â”œâ”€â”€ run_all.py                  # Script principal
â”‚   â”œâ”€â”€ auth/                       # AutenticaciÃ³n bÃ¡sica
â”‚   â”œâ”€â”€ cookies/                    # Cookies y sesiones
â”‚   â”œâ”€â”€ extraction/                 # JSON, XML, HTML
â”‚   â”œâ”€â”€ forms/                      # Formularios POST
â”‚   â”œâ”€â”€ errors/                     # Manejo de 403
â”‚   â”œâ”€â”€ redirects/                  # Redirecciones
â”‚   â””â”€â”€ out/                        # Salidas
â”‚
â”œâ”€â”€ 02_simulation_logs/             # MÃ³dulo 2: Logs sintÃ©ticos
â”‚   â”œâ”€â”€ generar_datos.py            # Generador JSONL
â”‚   â””â”€â”€ out/                        # Logs generados
â”‚
â”œâ”€â”€ 03_kpi_processing/              # MÃ³dulo 3: KPIs
â”‚   â”œâ”€â”€ calcular_kpis.py            # Calculador de KPIs
â”‚   â”œâ”€â”€ README.md                   # DocumentaciÃ³n
â”‚   â””â”€â”€ out/                        # KPIs (CSV)
â”‚
â”œâ”€â”€ 04_etl_pentaho/                 # MÃ³dulo 4: ETL
â”‚   â”œâ”€â”€ t_load_kpi.ktr              # TransformaciÃ³n
â”‚   â”œâ”€â”€ j_daily_kpi.kjb             # Job
â”‚   â”œâ”€â”€ db/                         # Base de datos
â”‚   â””â”€â”€ logs/                       # Logs
â”‚
â”œâ”€â”€ 05_reporting/                   # MÃ³dulo 5: Reportes
â”‚   â”œâ”€â”€ generar_reporte.py          # Generador HTML
â”‚   â””â”€â”€ out/                        # Reportes
â”‚
â”œâ”€â”€ setup_and_validate.py           # ValidaciÃ³n del ambiente
â”œâ”€â”€ requirements.txt                # Dependencias
â””â”€â”€ README.md                       # Este archivo
```

---

## ğŸš€ CÃ³mo ejecutar el pipeline

### EjecuciÃ³n completa (paso a paso)

```bash
# 1. Validar que todo estÃ¡ instalado
python setup_and_validate.py

# 2. Generar datos simulados
python 02_simulation_logs/generar_datos.py \
  --n_registros 500 \
  --seed 42

# 3. Calcular KPIs
python 03_kpi_processing/calcular_kpis.py \
  --input 02_simulation_logs/out/http_logs.jsonl \
  --output 03_kpi_processing/out/kpi_por_endpoint_dia.csv

# 4. Generar reporte HTML
python 05_reporting/generar_reporte.py \
  --input 03_kpi_processing/out/kpi_por_endpoint_dia.csv \
  --output 05_reporting/out/report/kpi_diario.html
```

### EjecuciÃ³n con valores por defecto

```bash
# Generar datos (500 registros, seed 42)
python 02_simulation_logs/generar_datos.py

# Procesar KPIs
python 03_kpi_processing/calcular_kpis.py

# Generar reporte
python 05_reporting/generar_reporte.py
```

---

## ğŸ“Š MÃ³dulos principales

### MÃ³dulo 02: GeneraciÃ³n de logs

Genera archivo JSONL con registros sintÃ©ticos de trÃ¡fico HTTP.

```bash
python 02_simulation_logs/generar_datos.py \
  --n_registros 1000 \
  --seed 42
```

**ParÃ¡metros:**
- `--n_registros`: NÃºmero de registros (default: 500)
- `--seed`: Semilla para reproducibilidad (default: 42)
- `--salida`: Ruta de salida (default: out/http_logs.jsonl)

---

### MÃ³dulo 03: Procesamiento de KPIs

Calcula indicadores de rendimiento diarios por endpoint.

```bash
python 03_kpi_processing/calcular_kpis.py \
  --input 02_simulation_logs/out/http_logs.jsonl \
  --output 03_kpi_processing/out/kpi_por_endpoint_dia.csv
```

**MÃ©tricas:**
- `requests_total`: Total de solicitudes
- `success_2xx`: Solicitudes exitosas (200-299)
- `client_4xx`: Errores del cliente (400-499)
- `server_5xx`: Errores del servidor (500-599)
- `parse_errors`: Registros con parse_result != "ok"
- `avg_elapsed_ms`: Tiempo promedio de respuesta
- `p90_elapsed_ms`: Percentil 90 de tiempo de respuesta

---

### MÃ³dulo 05: Reportes

Genera reporte HTML interactivo con tablas y grÃ¡ficos.

```bash
python 05_reporting/generar_reporte.py \
  --input 03_kpi_processing/out/kpi_por_endpoint_dia.csv \
  --output 05_reporting/out/report/kpi_diario.html \
  --umbral_p90 300
```

**Ver el reporte:**
```bash
# Windows
start 05_reporting/out/report/kpi_diario.html

# Linux
xdg-open 05_reporting/out/report/kpi_diario.html

# macOS
open 05_reporting/out/report/kpi_diario.html
```

---

## ğŸ”„ Flujo de datos completo

**Diagrama del Pipeline:**

```mermaid
%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#4299e1','primaryTextColor':'#fff','primaryBorderColor':'#2b6cb0','lineColor':'#4a5568','secondaryColor':'#48bb78','tertiaryColor':'#ecc94b'}}}%%

graph TB
  subgraph FASE1["ğŸŒ FASE 1: INGESTION"]
    A[IngestiÃ³n HTTP<br/>httpbin.org]
    A1[Auth bÃ¡sica<br/>Cookies/Sesiones<br/>JSON/XML/HTML]
    A --> A1
  end
    
  subgraph FASE2["ğŸ“Š FASE 2: SIMULATION"]
    B[GeneraciÃ³n de Logs<br/>generar_datos.py]
    B1[500+ registros JSONL<br/>Timestamps UTC<br/>Status codes realistas]
    B --> B1
  end
    
  subgraph FASE3["ğŸ“ˆ FASE 3: PROCESSING"]
    C[KPI Processing<br/>calcular_kpis.py]
    C1[Agregar por dÃ­a/endpoint<br/>Calc. percentiles p90<br/>MÃ©tricas 2xx/4xx/5xx]
    C --> C1
  end
    
  subgraph FASE4["âš™ï¸ FASE 4: ETL"]
    D[ETL Transformation<br/>t_load_kpi.ktr]
    D1[CSV Input<br/>Type Casting<br/>Filter Rows<br/>Table Output]
    D --> D1
  end
    
  subgraph FASE5["ğŸ“Š FASE 5: REPORTING"]
    E[Report Generator<br/>generar_reporte.py]
    E1[Tablas KPIs<br/>GrÃ¡ficos matplotlib<br/>HTML interactivo<br/>Alertas umbrales]
    E --> E1
  end
    
  DB[(ğŸ’¾ SQLite DB<br/>stg_kpi_endpoint_dia<br/>fct_kpi_endpoint_dia<br/>audit_etl_log)]
    
  OUTPUT{{"ğŸ¯ OUTPUT FINAL<br/>kpi_diario.html"}}
    
  F1[ğŸ“„ http_logs.jsonl]
  F2[ğŸ“„ kpi_por_endpoint_dia.csv]
    
  A1 -.simulate.-> B
  B1 -->|JSONL| F1
  F1 --> C
  C1 -->|CSV| F2
  F2 --> D
  F2 --> E
  D1 -->|load| DB
  E1 -->|generate| OUTPUT
    
  style A fill:#4299e1,stroke:#2b6cb0,stroke-width:3px,color:#fff
  style B fill:#48bb78,stroke:#2f855a,stroke-width:3px,color:#fff
  style C fill:#ecc94b,stroke:#d69e2e,stroke-width:3px,color:#fff
  style D fill:#f6ad55,stroke:#dd6b20,stroke-width:3px,color:#fff
  style E fill:#9f7aea,stroke:#6b46c1,stroke-width:3px,color:#fff
  style DB fill:#4a5568,stroke:#2d3748,stroke-width:3px,color:#fff
  style OUTPUT fill:#48bb78,stroke:#2f855a,stroke-width:4px,color:#fff
  style F1 fill:#fff,stroke:#718096,stroke-width:2px
  style F2 fill:#fff,stroke:#718096,stroke-width:2px
    
  style FASE1 fill:#ebf8ff,stroke:#3182ce,stroke-width:2px
  style FASE2 fill:#f0fff4,stroke:#38a169,stroke-width:2px
  style FASE3 fill:#fef5e7,stroke:#d69e2e,stroke-width:2px
  style FASE4 fill:#fef3c7,stroke:#f6ad55,stroke-width:2px
  style FASE5 fill:#faf5ff,stroke:#9f7aea,stroke-width:2px

```

---

### ğŸ“¸ Diagramas de ETL (Pentaho)

**TransformaciÃ³n (t_load_kpi.ktr):**

![TransformaciÃ³n t_load_kpi](04_etl_pentaho/diagramas/t_load_kpi.png)

**TransformaciÃ³n (t_load_kpi.ktr):**

![TransformaciÃ³n t_load_fact_kpi](04_etl_pentaho/diagramas/t_load_fact_kpi.png)

**Job (j_daily_kpi.kjb):**

![Job j_daily_kpi](04_etl_pentaho/diagramas/j_daily_kpi.png)

---

### Detalle: ETL con Pentaho (MÃ³dulo 04)

**Transformacion t_load_kpi.ktr (Pasos ejecutados):**

1. **CSV Input** â†’ Lee archivo `kpi_por_endpoint_dia.csv`
2. **Type Casting** â†’ Convierte tipos (fecha, entero, decimal)
3. **Filter Rows** â†’ Valida integridad de datos:
   - `requests_total > 0`
   - `p90_elapsed_ms >= avg_elapsed_ms`
4. **Table Output #1** â†’ Carga tabla staging (`stg_kpi_endpoint_dia`)
5. **Table Output #2** â†’ Carga tabla fact (`fct_kpi_endpoint_dia`)

**Job j_daily_kpi.kjb (Flujo completo):**

| Paso | Accion | Validacion |
|------|--------|------------|
| 1 | Ejecuta transformacion t_load_kpi.ktr | Verifica exito |
| 2 | Valida numero de filas en stg_kpi | Coincide con CSV |
| 3 | Valida numero de filas en fct_kpi | Igual a staging |
| 4 | Registra en log de auditoria | Timestamp + conteo |
| 5 | Envia email si hay errores | Opcional |

---

### SQLite Database (MÃ³dulo 04)

**Tabla Staging: stg_kpi_endpoint_dia**

```sql
CREATE TABLE stg_kpi_endpoint_dia (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  date_utc TEXT NOT NULL,
  endpoint_base TEXT NOT NULL,
  requests_total INTEGER NOT NULL,
  success_2xx INTEGER,
  client_4xx INTEGER,
  server_5xx INTEGER,
  parse_errors INTEGER DEFAULT 0,
  avg_elapsed_ms REAL,
  p90_elapsed_ms REAL,
  loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_stg_date ON stg_kpi_endpoint_dia(date_utc);
CREATE INDEX idx_stg_endpoint ON stg_kpi_endpoint_dia(endpoint_base);
```

**Tabla Fact: fct_kpi_endpoint_dia**

```sql
CREATE TABLE fct_kpi_endpoint_dia (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  date_utc TEXT NOT NULL,
  endpoint_base TEXT NOT NULL,
  requests_total INTEGER NOT NULL,
  success_2xx INTEGER,
  client_4xx INTEGER,
  server_5xx INTEGER,
  parse_errors INTEGER DEFAULT 0,
  avg_elapsed_ms REAL,
  p90_elapsed_ms REAL,
  loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_fct_date ON fct_kpi_endpoint_dia(date_utc);
CREATE INDEX idx_fct_endpoint ON fct_kpi_endpoint_dia(endpoint_base);
```

**Tabla de Auditoria: audit_etl_log**

```sql
CREATE TABLE audit_etl_log (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  job_name TEXT,
  execution_date TIMESTAMP,
  status TEXT,
  records_processed INTEGER,
  error_message TEXT,
  duration_seconds REAL
);
```

**Para crear las tablas en tu SQLite:**

```bash
sqlite3 04_etl_pentaho/kpis.db < 04_etl_pentaho/schema.sql
```

-- AuditorÃ­a: registro de cada carga
audit_etl_log (
  id, job_name, execution_date, records_loaded,
  records_expected, status, error_message
)
```

**Crear tablas:**
```bash
sqlite3 04_etl_pentaho/db/pipeline.db < 04_etl_pentaho/create_tables.sql
```

---

---

## ğŸš¨ Troubleshooting

### "ModuleNotFoundError: No module named 'pandas'"

```bash
pip install -r requirements.txt
```

### "FileNotFoundError: No existe el input JSONL"

```bash
cd 02_simulation_logs
python generar_datos.py --salida out/http_logs.jsonl
```

### "JSON mal formado en lÃ­nea X"

```bash
# Regenera los datos
python 02_simulation_logs/generar_datos.py --n_registros 100
```

### "No existe el archivo de reporte"

```bash
mkdir -p 05_reporting/out/report
python 05_reporting/generar_reporte.py
```

---

## ğŸ“š TecnologÃ­as utilizadas

| CategorÃ­a | TecnologÃ­a |
|-----------|------------|
| Backend | Python 3.8+ |
| HTTP | requests |
| Parsing | beautifulsoup4, lxml |
| Datos sintÃ©ticos | Faker |
| AnÃ¡lisis | pandas, numpy |
| VisualizaciÃ³n | matplotlib |
| ETL | Pentaho Data Integration (opcional) |
| BD | SQLite (opcional) |

---

## ğŸ“ FAQ

**Â¿Puedo usar esto en producciÃ³n?**  
SÃ­. Los mÃ³dulos 02, 03, 05 estÃ¡n listos. El mÃ³dulo 04 requiere instalaciÃ³n de Pentaho.

**Â¿CÃ³mo cambio el umbral de p90?**  
```bash
python 05_reporting/generar_reporte.py --umbral_p90 500
```

**Â¿QuÃ© significa p90_elapsed_ms?**  
Es el percentil 90 del tiempo de respuesta. El 90% de solicitudes tardÃ³ menos que este valor.

**Â¿CÃ³mo aÃ±ado mÃ¡s endpoints?**  
Edita `02_simulation_logs/generar_datos.py` y modifica la lista `ENDPOINTS`.

---

## ğŸ“„ Licencia

Este proyecto fue desarrollado como parte de una **prueba tÃ©cnica** de Data Engineering.

Derechos de uso:
- âœ… Uso educativo y de demostraciÃ³n
- âœ… ModificaciÃ³n y distribuciÃ³n con atribuciÃ³n
- âœ… Uso en entornos de desarrollo y testing
- âš ï¸ No incluye garantÃ­as de soporte para producciÃ³n

---

## âœ‰ï¸ Contacto

**Desarrollador:** Milton QuiÃ±onez  
**GitHub:** [@Milton-RQM](https://github.com/Milton-RQM)  
**Email:** miltonrene530@gmail.com  
**Proyecto:** [client-automated-HTTP](https://github.com/Milton-RQM/client-automated-HTTP)

Para preguntas o sugerencias:
- ğŸ“§ Email: miltonrene530@gmail.com
- ğŸ’¬ Issues: Abre un issue en el repositorio de GitHub

---

## ğŸ“š Referencias

### DocumentaciÃ³n oficial

- **httpbin.org**: [httpbin.org/docs](https://httpbin.org/docs)
- **Python Requests**: [docs.python-requests.org](https://docs.python-requests.org/)
- **Beautiful Soup**: [bs4.readthedocs.io](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- **Faker (datos sintÃ©ticos)**: [faker.readthedocs.io](https://faker.readthedocs.io/)
- **Pandas**: [pandas.pydata.org](https://pandas.pydata.org/)
- **NumPy**: [numpy.org](https://numpy.org/)
- **Matplotlib**: [matplotlib.org](https://matplotlib.org/)

### Pentaho Data Integration

- **DocumentaciÃ³n oficial**: [help.hitachivantara.com/Pentaho](https://help.hitachivantara.com/Documentation/Software/Pentaho/9.0)
- **Descargar PDI**: [pentaho.com/download](https://www.pentaho.com/download)
- **Spoon User Guide**: [GuÃ­a de usuario](https://help.hitachivantara.com/Documentation/Software/Pentaho/9.0/en)

### Conceptos complementarios

- **ETL (Extract, Transform, Load)**: [wikipedia.org/ETL](https://en.wikipedia.org/wiki/Extract,_transform,_load)
- **KPIs (Key Performance Indicators)**: [wikipedia.org/KPI](https://en.wikipedia.org/wiki/Key_performance_indicator)
- **Percentiles estadÃ­sticos**: [wikipedia.org/Percentile](https://en.wikipedia.org/wiki/Percentile)

---

## ğŸ“ Aprendizajes clave

Este proyecto demuestra:

1. **IntegraciÃ³n de APIs**: Consumo de endpoints HTTP con requests
2. **Calidad de datos**: ValidaciÃ³n, normalizaciÃ³n y limpieza
3. **IngenierÃ­a ETL**: Pipelines automatizados con Pentaho
4. **AnÃ¡lisis de datos**: KPIs, percentiles, agregaciones
5. **VisualizaciÃ³n**: GrÃ¡ficos y reportes HTML interactivos
6. **AutomatizaciÃ³n**: Scripts reproducibles con parÃ¡metros CLI
7. **Buenas prÃ¡cticas**: DocumentaciÃ³n, errores, validaciÃ³n

---

## âœ¨ Mejoras futuras

- [ ] Agregar autenticaciÃ³n a API de reportes
- [ ] Implementar dashboard en tiempo real con Plotly
- [ ] IntegraciÃ³n con Apache Airflow para orquestaciÃ³n
- [ ] Alertas automÃ¡ticas por email en casos de anomalÃ­as
- [ ] HistÃ³rico de KPIs con consultas de tendencias
- [ ] API REST para consultar KPIs
- [ ] DockerizaciÃ³n del pipeline completo
- [ ] Pruebas unitarias y de integraciÃ³n

---

**Ãšltima actualizaciÃ³n:** 2026-02-06  
**VersiÃ³n:** 1.0.0  
**Estado:** âœ… Listo para ejecuciÃ³n en cualquier equipo con Python 3.10+



